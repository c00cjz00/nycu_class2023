{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## INSTALL PACKAGE\n",
        "!pip install -q markdown pinecone-client openai gdown xformers ctransformers tokenizers transformers accelerate langchain chainlit sentence_transformers chromadb unstructured PyPDF2 pypdf bitsandbytes faiss_cpu faiss_gpu huggingface_hub hf_transfer optimum -q\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  -q # Use cu117 if on CUDA 11.7"
      ],
      "metadata": {
        "id": "A6EMFg6fTWei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U9ApDM1S_KW"
      },
      "outputs": [],
      "source": [
        "# 01: Configure\n",
        "pdf_file='Medical_Chatbot.pdf'\n",
        "PINECONE_API_KEY='20163887-a4fa-44e7-98d2-ab1eb38937f6'\n",
        "PINECONE_API_ENV='gcp-starter'\n",
        "index_name=\"cjz-medical\"\n",
        "Embeddings_ID=\"sentence-transformers/all-MiniLM-L6-v2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 02.1: LOAD LIBRARY\n",
        "from torch import cuda, bfloat16, float16\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA\n",
        "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig, pipeline, TextStreamer"
      ],
      "metadata": {
        "id": "KmNxwWAcz4QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 02.2: Load LIBRARY\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import transformers\n",
        "import torch\n",
        "import pinecone"
      ],
      "metadata": {
        "id": "M5Yy3u4NtvTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# 3. Download model\n",
        "mkdir -p Llama-7B-Chat-GPTQ\n",
        "HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Llama-2-7B-Chat-GPTQ --local-dir Llama-7B-Chat-GPTQ --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "id": "e6yk1vFj0Api"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 04: Embeddings 模型 384維度\n",
        "embeddings=HuggingFaceEmbeddings(model_name=Embeddings_ID)"
      ],
      "metadata": {
        "id": "ximtQ0_NzIYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 05: 連線 pinecone 向量資料庫\n",
        "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_API_ENV)\n",
        "docsearch=Pinecone.from_existing_index(index_name, embeddings)"
      ],
      "metadata": {
        "id": "8-hnihKvzIgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 06: 搜尋 pinecone 向量資料庫, 列出前三名\n",
        "query = \"What are Allergies\"\n",
        "docs=docsearch.similarity_search(query, k=3)\n",
        "docs"
      ],
      "metadata": {
        "id": "vpbcnVMdzIlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 07: LLM模型\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "MODEL_ID = \"./Llama-7B-Chat-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0})"
      ],
      "metadata": {
        "id": "eNoMSe5izIpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 08. SET QA Search module\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "retriever = docsearch.as_retriever(search_kwargs={\"k\": 3})\n",
        "retrieval_qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "AYv-kczKzItJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 09. DO QA Search\n",
        "query = \"What are Allergies\"\n",
        "llm_response = retrieval_qa_chain(query)\n",
        "print(llm_response['query'])\n",
        "print(llm_response['result'])\n",
        "print(llm_response['source_documents'])"
      ],
      "metadata": {
        "id": "GfZ9iC4VzIxg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}