{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6EMFg6fTWei"
   },
   "source": [
    "## 實作案例: 請先切換kernel 於您新安裝的 kernel環境\n",
    "## <span style=\"color:red\">Change to kernel:   Image_XXXXXXXXXXl</span>.\n",
    "\n",
    "https://python.langchain.com.cn/docs/get_started/quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始環境設定\n",
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin:/usr/ubuntu_bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8U9ApDM1S_KW"
   },
   "outputs": [],
   "source": [
    "# 01: Configure\n",
    "pdf_file='Medical_Chatbot.pdf'\n",
    "PINECONE_API_KEY=''\n",
    "PINECONE_API_ENV='gcp-starter'\n",
    "index_name=\"cjz-medical\"\n",
    "Embeddings_ID=\"/work/u00cjz00/slurm_jobs/github/models/embedding/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmNxwWAcz4QF"
   },
   "outputs": [],
   "source": [
    "### 02.1: LOAD LIBRARY\n",
    "from torch import cuda, bfloat16, float16\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig, pipeline, TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5Yy3u4NtvTa"
   },
   "outputs": [],
   "source": [
    "# 02.2: Load LIBRARY\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import transformers\n",
    "import torch\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "e6yk1vFj0Api"
   },
   "source": [
    "%%bash\n",
    "# 3. Download model\n",
    "mkdir -p Llama-7B-Chat-GPTQ\n",
    "HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Llama-2-7B-Chat-GPTQ --local-dir Llama-7B-Chat-GPTQ --local-dir-use-symlinks False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ximtQ0_NzIYx"
   },
   "outputs": [],
   "source": [
    "# 04: Embeddings 模型 384維度\n",
    "embeddings=HuggingFaceEmbeddings(model_name=Embeddings_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-hnihKvzIgW"
   },
   "outputs": [],
   "source": [
    "# 05: 連線 pinecone 向量資料庫\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_API_ENV)\n",
    "docsearch=Pinecone.from_existing_index(index_name, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpbcnVMdzIlF"
   },
   "outputs": [],
   "source": [
    "# 06: 搜尋 pinecone 向量資料庫, 列出前三名\n",
    "query = \"What are Allergies\"\n",
    "docs=docsearch.similarity_search(query, k=3)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNoMSe5izIpR"
   },
   "outputs": [],
   "source": [
    "# 07: LLM模型\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "MODEL_ID = \"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7B-Chat-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYv-kczKzItJ"
   },
   "outputs": [],
   "source": [
    "## 08. SET QA Search module\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "retriever = docsearch.as_retriever(search_kwargs={\"k\": 3})\n",
    "retrieval_qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                  chain_type=\"stuff\",\n",
    "                                  retriever=retriever,\n",
    "                                  return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfZ9iC4VzIxg"
   },
   "outputs": [],
   "source": [
    "## 09. DO QA Search\n",
    "query = \"What are Allergies\"\n",
    "llm_response = retrieval_qa_chain(query)\n",
    "print(llm_response['query'])\n",
    "print(llm_response['result'])\n",
    "print(llm_response['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Image_S_work-course2024_pytorch_2.1.0-cuda11.8-cudnn8-devel",
   "language": "python",
   "name": "s_work-course2024_pytorch_2.1.0-cuda11.8-cudnn8-devel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
