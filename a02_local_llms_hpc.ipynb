{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6EMFg6fTWei"
   },
   "outputs": [],
   "source": [
    "## 實作案例: 請先切換kernel 於您新安裝的 kernel環境\n",
    "## <span style=\"color:red\">Change to kernel:   Image_XXXXXXXXXXl</span>.\n",
    "\n",
    "https://python.langchain.com.cn/docs/get_started/quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始環境設定\n",
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin:/usr/ubuntu_bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8U9ApDM1S_KW"
   },
   "outputs": [],
   "source": [
    "# LOAD LIBRARY\n",
    "from torch import cuda, bfloat16, float16\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig, pipeline, TextStreamer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "yxxlzaLHUo-T"
   },
   "source": [
    "%%bash\n",
    "# Download model\n",
    "mkdir -p Llama-7B-Chat-GPTQ\n",
    "HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Llama-2-7B-Chat-GPTQ --local-dir Llama-7B-Chat-GPTQ --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJha7llxV33l"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "MODEL_ID = \"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7B-Chat-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "xfsGaWT9W8WG",
    "outputId": "7cc3e620-9fe0-4301-f3a3-94858b67769d"
   },
   "outputs": [],
   "source": [
    "llm(\"Explain the difference between ChatGPT and open source LLMs in a couple of lines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "luTLJ1NlXEVm"
   },
   "outputs": [],
   "source": [
    "### Prompt Template\n",
    "prompt_template = '''\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "{question} [/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74pXeVdzVliw",
    "outputId": "5e7bc74c-7bad-44b8-af97-354fbf0c605e"
   },
   "outputs": [],
   "source": [
    "question = \"Explain what are Deep Neural Networks in 2-3 sentences\"\n",
    "print(prompt.format(question=question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "u9U4w7c6W-D5",
    "outputId": "9ac605df-74d4-4e22-f0b9-a3c0dd09693b"
   },
   "outputs": [],
   "source": [
    "llm(prompt.format(question=question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fL7Xm-hWEco"
   },
   "outputs": [],
   "source": [
    "## Create Chain01\n",
    "from langchain.chains import LLMChain\n",
    "chain01 = LLMChain(llm=llm, prompt=prompt)\n",
    "result01 = chain01.run(question=question)\n",
    "print(result01.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-X1HOzSWG-u",
    "outputId": "8d0154f0-27b0-4aa1-d975-27f1062f1315"
   },
   "outputs": [],
   "source": [
    "## Create Chain02\n",
    "### Prompt Template for summary\n",
    "prompt_template = \"<s>[INST] Use the summary {summary} and give 3 examples of practical applications with 1 sentence explaining each [/INST]\"\n",
    "\n",
    "prompt02 = PromptTemplate(\n",
    "    input_variables=[\"summary\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "chain02 = LLMChain(llm=llm, prompt=prompt02)\n",
    "print(prompt02.format(summary=result01))\n",
    "result02 = chain02.run(result01)\n",
    "print(result02.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Td36DOpWWYJR",
    "outputId": "1d106681-6a6a-41cb-97f0-0631e0390fae"
   },
   "outputs": [],
   "source": [
    "## Chaining Chains\n",
    "## multi_chain\n",
    "multi_chain = SimpleSequentialChain(chains=[chain01, chain02], verbose=True)\n",
    "result_mutiple = multi_chain.run(question)\n",
    "print(result_mutiple.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61PI5raXYA23",
    "outputId": "b5d00d5d-86e0-45e7-bffc-ba3a975d39a1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4Xh3hwdYGgl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Image_S_work-course2024_pytorch_2.1.0-cuda11.8-cudnn8-devel",
   "language": "python",
   "name": "s_work-course2024_pytorch_2.1.0-cuda11.8-cudnn8-devel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
