{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0PlUF-R2rmx"
   },
   "source": [
    "###**This notebook is deprecated and will no longer be updated.**\n",
    "\n",
    "###**[The official notebook (made by oobabooga) is located here](https://colab.research.google.com/github/oobabooga/text-generation-webui/blob/main/Colab-TextGen-GPU.ipynb). Use it instead.**\n",
    "\n",
    "####**Alternatively, try the [official KoboldCpp colab](https://colab.research.google.com/github/lostruins/koboldcpp/blob/concedo/colab.ipynb). It's fast, lightweight, and easy to use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95
    },
    "id": "tmzc69IfrZ-g",
    "outputId": "d1b48303-9a05-4215-ba1d-1122a1cb5788"
   },
   "outputs": [],
   "source": [
    "#@markdown\n",
    "%%html\n",
    "<b>ðŸ‘‡ Press play on this music player to keep the current tab alive. Keep this page open and occasionally check for CAPTCHA's so that you are not disconnected.</b><br/><br />\n",
    "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "UecGsZ88rsOF"
   },
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from IPython.utils import capture\n",
    "from google.colab import files, drive\n",
    "\n",
    "#PARAMS\n",
    "#@markdown ðŸ‘ˆ Press this button after configuring the settings below to start the installation process. The links will appear at the bottom of this page after a few minutes.\n",
    "\n",
    "Model = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\" #@param [\"https://huggingface.co/TheBloke/LLaMA2-13B-Tiefighter-GPTQ\", \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\", \"https://huggingface.co/TheBloke/Mistral-ClaudeLimaRP-v3-7B-GPTQ\", \"https://huggingface.co/TheBloke/Mythalion-13B-GPTQ\", \"https://huggingface.co/TheBloke/MythoMax-L2-13B-GPTQ\",    \"https://huggingface.co/TheBloke/MythoMist-7B-GPTQ\", \"https://huggingface.co/TheBloke/openchat_3.5-GPTQ\", \"https://huggingface.co/TheBloke/ReMM-SLERP-L2-13B-GPTQ\", \"https://huggingface.co/TheBloke/Toppy-M-7B-GPTQ\",  \"https://huggingface.co/TheBloke/Xwin-MLewd-13B-v0.2-GPTQ\",   \"https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ\"]{allow-input: true}\n",
    "#@markdown > <font color=\"gray\">Enter any model URL from [Hugging Face](https://huggingface.co/models). Tap the arrow to view some examples.\n",
    "Branch = \"\" #@param {type:\"string\"}\n",
    "#@markdown > <font color=\"gray\">You can specify a valid model branch here. Defaults to ``main`` if left blank.\n",
    "Flags = \"--share --api --public-api --chat-buttons\" #@param {type:\"string\"}\n",
    "#@markdown > <font color=\"gray\">You can write any valid [command-line flags](https://github.com/oobabooga/text-generation-webui/blob/main/README.md#basic-settings) or [extensions](https://github.com/oobabooga/text-generation-webui/blob/main/docs/Extensions.md) here.\n",
    "Google_Drive = False #@param {type:\"boolean\"}\n",
    "#@markdown > <font color=\"gray\">Remember that models are very large, and free Google Drive only provides 15GB. [Manage your Drive space here.](https://drive.google.com/drive/my-drive)\n",
    "Debug = False #@param {type:\"boolean\"}\n",
    "#@markdown > <font color=\"gray\">Shows the installation console log if checked.\n",
    "\n",
    "def message_a():\n",
    "\tclear_output(wait = True)\n",
    "\tprint(f\"\\033[1;32;1m\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nBeginning installation. This will take a few minutes.\\n\\nInstalling WebUI...\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\033[0;37;0m\")\n",
    "\tdisplay(HTML(\"<img src='https://i.redd.it/cf74oy8te80c1.gif' style='vertical-align:middle;margin:0px 50px'/>\"))\n",
    "def message_b():\n",
    "\tclear_output(wait = True)\n",
    "\tprint(f\"\\033[1;32;1m\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nBeginning installation. This will take a few minutes.\\n\\nWebUI installed. Downloading model...\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\033[0;37;0m\")\n",
    "\tdisplay(HTML(\"<img src='https://i.redd.it/cf74oy8te80c1.gif' style='vertical-align:middle;margin:0px 50px'/>\"))\n",
    "def message_c():\n",
    "\tclear_output(wait = True)\n",
    "\tprint(f\"\\033[1;32;1m\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nInstallation complete. To enter the WebUI, click on the gradio link that will appear below.\\n\\nFor SillyTavern users, copy the Blocking API URL and paste into SillyTavern's API settings.\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\033[0;37;0m\")\n",
    "\n",
    "# Install WebUI\n",
    "def install_webui():\n",
    "\tif os.path.exists(repo_dir):\n",
    "\t\t%cd {repo_dir}\n",
    "\t\t!git pull\n",
    "\telse:\n",
    "\t\t!git clone https://github.com/oobabooga/text-generation-webui.git\n",
    "\t%cd {repo_dir}\n",
    "\n",
    "\t# Install WebUI requirements\n",
    "\ttorver = torch.__version__\n",
    "\tis_cuda118 = '+cu118' in torver  # 2.1.0+cu118\n",
    "\tis_cuda117 = '+cu117' in torver  # 2.0.1+cu117\n",
    "\ttextgen_requirements = open('requirements.txt').read().splitlines()\n",
    "\tif is_cuda117:\n",
    "\t\ttextgen_requirements = [req.replace('+cu121', '+cu117').replace('+cu122', '+cu117').replace('torch2.1', 'torch2.0') for req in textgen_requirements]\n",
    "\telif is_cuda118:\n",
    "\t\ttextgen_requirements = [req.replace('+cu121', '+cu118').replace('+cu122', '+cu118') for req in textgen_requirements]\n",
    "\twith open('temp_requirements.txt', 'w') as file:\n",
    "\t\tfile.write('\\n'.join(textgen_requirements))\n",
    "\t!pip install -r temp_requirements.txt --upgrade\n",
    "\n",
    "\t# Install extension requirements\n",
    "\tif 'deepspeed' in Flags:\n",
    "\t\t!pip install -U mpi4py\n",
    "\t\t!pip install -U deepspeed\n",
    "\tif 'xformers' in Flags:\n",
    "\t\t!pip install xformers\n",
    "\tif 'api' in Flags:\n",
    "\t\t!pip install -r extensions/openai/requirements.txt\n",
    "\tif 'google_translate' in Flags:\n",
    "\t\t!pip install -r extensions/google_translate/requirements.txt\n",
    "\tif 'superbooga' in Flags:\n",
    "\t\t!pip install -r extensions/superbooga/requirements.txt\n",
    "\tif 'silero_tts' in Flags:\n",
    "\t\t!pip install -r extensions/silero_tts/requirements.txt\n",
    "\tif 'elevenlabs_tts' in Flags:\n",
    "\t\t!pip install -r extensions/elevenlabs_tts/requirements.txt\n",
    "\tif 'whisper_stt' in Flags:\n",
    "\t\t!pip install -r extensions/whisper_stt/requirements.txt\n",
    "\tif 'openai' in Flags:\n",
    "\t\t!pip install -r extensions/openai/requirements.txt\n",
    "\tif 'ngrok' in Flags:\n",
    "\t\t!pip install -r extensions/ngrok/requirements.txt\n",
    "\n",
    "\ttry:\n",
    "\t\timport flash_attn\n",
    "\texcept:\n",
    "\t\t!pip uninstall -y flash_attn\n",
    "\n",
    "# Mount Google Drive if enabled\n",
    "if Google_Drive:\n",
    "\tdrive.mount('/content/drive')\n",
    "\t%cd /content/drive/MyDrive\n",
    "\trepo_dir = '/content/drive/MyDrive/text-generation-webui'\n",
    "\tmodel_dir = '/content/drive/MyDrive/text-generation-webui/models'\n",
    "else:\n",
    "\t%cd /content\n",
    "\trepo_dir = '/content/text-generation-webui'\n",
    "\tmodel_dir = '/content/text-generation-webui/models'\n",
    "if not Debug:\n",
    "\tmessage_a()\n",
    "\twith capture.capture_output() as cap:\n",
    "\t\tinstall_webui()\n",
    "else:\n",
    "\tinstall_webui()\n",
    "\n",
    "# Download model\n",
    "def install_model():\n",
    "\tglobal output_folder\n",
    "\tmodel_url = Model.strip()\n",
    "\tif model_url != \"\":\n",
    "\t\tif not model_url.startswith('http'):\n",
    "\t\t\tmodel_url = 'https://huggingface.co/' + model_url\n",
    "\t\turl_parts = model_url.strip('/').strip().split('/')\n",
    "\t\toutput_folder = f\"{url_parts[-2]}_{url_parts[-1]}\"\n",
    "\t\tbranch = Branch.strip('\"\\' ')\n",
    "\t\tif Branch.strip() not in ['', 'main']:\n",
    "\t\t\toutput_folder += f\"_{branch}\"\n",
    "\t\t\t!python download-model.py {model_url} --branch {branch}\n",
    "\t\telse:\n",
    "\t\t\t!python download-model.py {model_url}\n",
    "\telse:\n",
    "\t\toutput_folder = \"\"\n",
    "\n",
    "if not Debug:\n",
    "\tmessage_b()\n",
    "\twith capture.capture_output() as cap:\n",
    "\t\tinstall_model()\n",
    "else:\n",
    "\tinstall_model()\n",
    "\n",
    "# Run WebUI\n",
    "if not Debug:\n",
    "\tmessage_c()\n",
    "if 'deepspeed' in Flags:\n",
    "\tcmd = f\"deepspeed --num_gpus=1 server.py\"\n",
    "else:\n",
    "\tcmd = f\"HF_TOKEN='hf_xxxxxxxxxxxxxx' python server.py\"\n",
    "if output_folder != \"\":\n",
    "    cmd += f\" --model {output_folder}\"\n",
    "cmd += f\" {Flags}\"\n",
    "print(cmd)\n",
    "!$cmd"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
