{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###**This notebook is deprecated and will no longer be updated.**\n",
        "\n",
        "###**[The official notebook (made by oobabooga) is located here](https://colab.research.google.com/github/oobabooga/text-generation-webui/blob/main/Colab-TextGen-GPU.ipynb). Use it instead.**\n",
        "\n",
        "####**Alternatively, try the [official KoboldCpp colab](https://colab.research.google.com/github/lostruins/koboldcpp/blob/concedo/colab.ipynb). It's fast, lightweight, and easy to use.**"
      ],
      "metadata": {
        "id": "e0PlUF-R2rmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "tmzc69IfrZ-g",
        "outputId": "d1b48303-9a05-4215-ba1d-1122a1cb5788",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<b>ðŸ‘‡ Press play on this music player to keep the current tab alive. Keep this page open and occasionally check for CAPTCHA's so that you are not disconnected.</b><br/><br />\n",
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown\n",
        "%%html\n",
        "<b>ðŸ‘‡ Press play on this music player to keep the current tab alive. Keep this page open and occasionally check for CAPTCHA's so that you are not disconnected.</b><br/><br />\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UecGsZ88rsOF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import os, torch\n",
        "from IPython.display import clear_output, display, HTML\n",
        "from IPython.utils import capture\n",
        "from google.colab import files, drive\n",
        "\n",
        "#PARAMS\n",
        "#@markdown ðŸ‘ˆ Press this button after configuring the settings below to start the installation process. The links will appear at the bottom of this page after a few minutes.\n",
        "\n",
        "Model = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\" #@param [\"https://huggingface.co/TheBloke/LLaMA2-13B-Tiefighter-GPTQ\", \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\", \"https://huggingface.co/TheBloke/Mistral-ClaudeLimaRP-v3-7B-GPTQ\", \"https://huggingface.co/TheBloke/Mythalion-13B-GPTQ\", \"https://huggingface.co/TheBloke/MythoMax-L2-13B-GPTQ\",    \"https://huggingface.co/TheBloke/MythoMist-7B-GPTQ\", \"https://huggingface.co/TheBloke/openchat_3.5-GPTQ\", \"https://huggingface.co/TheBloke/ReMM-SLERP-L2-13B-GPTQ\", \"https://huggingface.co/TheBloke/Toppy-M-7B-GPTQ\",  \"https://huggingface.co/TheBloke/Xwin-MLewd-13B-v0.2-GPTQ\",   \"https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ\"]{allow-input: true}\n",
        "#@markdown > <font color=\"gray\">Enter any model URL from [Hugging Face](https://huggingface.co/models). Tap the arrow to view some examples.\n",
        "Branch = \"\" #@param {type:\"string\"}\n",
        "#@markdown > <font color=\"gray\">You can specify a valid model branch here. Defaults to ``main`` if left blank.\n",
        "Flags = \"--share --api --public-api --chat-buttons\" #@param {type:\"string\"}\n",
        "#@markdown > <font color=\"gray\">You can write any valid [command-line flags](https://github.com/oobabooga/text-generation-webui/blob/main/README.md#basic-settings) or [extensions](https://github.com/oobabooga/text-generation-webui/blob/main/docs/Extensions.md) here.\n",
        "Google_Drive = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Remember that models are very large, and free Google Drive only provides 15GB. [Manage your Drive space here.](https://drive.google.com/drive/my-drive)\n",
        "Debug = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Shows the installation console log if checked.\n",
        "\n",
        "def message_a():\n",
        "\tclear_output(wait = True)\n",
        "\tprint(f\"\\033[1;32;1m\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nBeginning installation. This will take a few minutes.\\n\\nInstalling WebUI...\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\033[0;37;0m\")\n",
        "\tdisplay(HTML(\"<img src='https://i.redd.it/cf74oy8te80c1.gif' style='vertical-align:middle;margin:0px 50px'/>\"))\n",
        "def message_b():\n",
        "\tclear_output(wait = True)\n",
        "\tprint(f\"\\033[1;32;1m\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nBeginning installation. This will take a few minutes.\\n\\nWebUI installed. Downloading model...\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\033[0;37;0m\")\n",
        "\tdisplay(HTML(\"<img src='https://i.redd.it/cf74oy8te80c1.gif' style='vertical-align:middle;margin:0px 50px'/>\"))\n",
        "def message_c():\n",
        "\tclear_output(wait = True)\n",
        "\tprint(f\"\\033[1;32;1m\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nInstallation complete. To enter the WebUI, click on the gradio link that will appear below.\\n\\nFor SillyTavern users, copy the Blocking API URL and paste into SillyTavern's API settings.\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\033[0;37;0m\")\n",
        "\n",
        "# Install WebUI\n",
        "def install_webui():\n",
        "\tif os.path.exists(repo_dir):\n",
        "\t\t%cd {repo_dir}\n",
        "\t\t!git pull\n",
        "\telse:\n",
        "\t\t!git clone https://github.com/oobabooga/text-generation-webui.git\n",
        "\t%cd {repo_dir}\n",
        "\n",
        "\t# Install WebUI requirements\n",
        "\ttorver = torch.__version__\n",
        "\tis_cuda118 = '+cu118' in torver  # 2.1.0+cu118\n",
        "\tis_cuda117 = '+cu117' in torver  # 2.0.1+cu117\n",
        "\ttextgen_requirements = open('requirements.txt').read().splitlines()\n",
        "\tif is_cuda117:\n",
        "\t\ttextgen_requirements = [req.replace('+cu121', '+cu117').replace('+cu122', '+cu117').replace('torch2.1', 'torch2.0') for req in textgen_requirements]\n",
        "\telif is_cuda118:\n",
        "\t\ttextgen_requirements = [req.replace('+cu121', '+cu118').replace('+cu122', '+cu118') for req in textgen_requirements]\n",
        "\twith open('temp_requirements.txt', 'w') as file:\n",
        "\t\tfile.write('\\n'.join(textgen_requirements))\n",
        "\t!pip install -r temp_requirements.txt --upgrade\n",
        "\n",
        "\t# Install extension requirements\n",
        "\tif 'deepspeed' in Flags:\n",
        "\t\t!pip install -U mpi4py\n",
        "\t\t!pip install -U deepspeed\n",
        "\tif 'xformers' in Flags:\n",
        "\t\t!pip install xformers\n",
        "\tif 'api' in Flags:\n",
        "\t\t!pip install -r extensions/openai/requirements.txt\n",
        "\tif 'google_translate' in Flags:\n",
        "\t\t!pip install -r extensions/google_translate/requirements.txt\n",
        "\tif 'superbooga' in Flags:\n",
        "\t\t!pip install -r extensions/superbooga/requirements.txt\n",
        "\tif 'silero_tts' in Flags:\n",
        "\t\t!pip install -r extensions/silero_tts/requirements.txt\n",
        "\tif 'elevenlabs_tts' in Flags:\n",
        "\t\t!pip install -r extensions/elevenlabs_tts/requirements.txt\n",
        "\tif 'whisper_stt' in Flags:\n",
        "\t\t!pip install -r extensions/whisper_stt/requirements.txt\n",
        "\tif 'openai' in Flags:\n",
        "\t\t!pip install -r extensions/openai/requirements.txt\n",
        "\tif 'ngrok' in Flags:\n",
        "\t\t!pip install -r extensions/ngrok/requirements.txt\n",
        "\n",
        "\ttry:\n",
        "\t\timport flash_attn\n",
        "\texcept:\n",
        "\t\t!pip uninstall -y flash_attn\n",
        "\n",
        "# Mount Google Drive if enabled\n",
        "if Google_Drive:\n",
        "\tdrive.mount('/content/drive')\n",
        "\t%cd /content/drive/MyDrive\n",
        "\trepo_dir = '/content/drive/MyDrive/text-generation-webui'\n",
        "\tmodel_dir = '/content/drive/MyDrive/text-generation-webui/models'\n",
        "else:\n",
        "\t%cd /content\n",
        "\trepo_dir = '/content/text-generation-webui'\n",
        "\tmodel_dir = '/content/text-generation-webui/models'\n",
        "if not Debug:\n",
        "\tmessage_a()\n",
        "\twith capture.capture_output() as cap:\n",
        "\t\tinstall_webui()\n",
        "else:\n",
        "\tinstall_webui()\n",
        "\n",
        "# Download model\n",
        "def install_model():\n",
        "\tglobal output_folder\n",
        "\tmodel_url = Model.strip()\n",
        "\tif model_url != \"\":\n",
        "\t\tif not model_url.startswith('http'):\n",
        "\t\t\tmodel_url = 'https://huggingface.co/' + model_url\n",
        "\t\turl_parts = model_url.strip('/').strip().split('/')\n",
        "\t\toutput_folder = f\"{url_parts[-2]}_{url_parts[-1]}\"\n",
        "\t\tbranch = Branch.strip('\"\\' ')\n",
        "\t\tif Branch.strip() not in ['', 'main']:\n",
        "\t\t\toutput_folder += f\"_{branch}\"\n",
        "\t\t\t!python download-model.py {model_url} --branch {branch}\n",
        "\t\telse:\n",
        "\t\t\t!python download-model.py {model_url}\n",
        "\telse:\n",
        "\t\toutput_folder = \"\"\n",
        "\n",
        "if not Debug:\n",
        "\tmessage_b()\n",
        "\twith capture.capture_output() as cap:\n",
        "\t\tinstall_model()\n",
        "else:\n",
        "\tinstall_model()\n",
        "\n",
        "# Run WebUI\n",
        "if not Debug:\n",
        "\tmessage_c()\n",
        "if 'deepspeed' in Flags:\n",
        "\tcmd = f\"deepspeed --num_gpus=1 server.py\"\n",
        "else:\n",
        "\tcmd = f\"python server.py\"\n",
        "if output_folder != \"\":\n",
        "    cmd += f\" --model {output_folder}\"\n",
        "cmd += f\" {Flags}\"\n",
        "print(cmd)\n",
        "!$cmd"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}